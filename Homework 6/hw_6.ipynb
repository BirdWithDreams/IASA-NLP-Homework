{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install evaluate\n",
    "!pip install py7zr\n",
    "!pip install sacrebleu\n",
    "!pip install rouge_score"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-05T15:05:26.127772Z",
     "iopub.execute_input": "2023-11-05T15:05:26.128431Z",
     "iopub.status.idle": "2023-11-05T15:06:19.041597Z",
     "shell.execute_reply.started": "2023-11-05T15:05:26.128401Z",
     "shell.execute_reply": "2023-11-05T15:06:19.040633Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m6.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\nCollecting py7zr\n  Downloading py7zr-0.20.6-py3-none-any.whl (66 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m66.7/66.7 kB\u001B[0m \u001B[31m6.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.6.7)\nCollecting pycryptodomex>=3.6.6 (from py7zr)\n  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m47.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n\u001B[?25hCollecting pyzstd>=0.14.4 (from py7zr)\n  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m412.3/412.3 kB\u001B[0m \u001B[31m35.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting pyppmd<1.1.0,>=0.18.1 (from py7zr)\n  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m138.8/138.8 kB\u001B[0m \u001B[31m17.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting pybcj>=0.6.0 (from py7zr)\n  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.7/49.7 kB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting multivolumefile>=0.2.3 (from py7zr)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nCollecting brotli>=1.0.9 (from py7zr)\n  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m31.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0mm\n\u001B[?25hCollecting inflate64>=0.3.1 (from py7zr)\n  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m93.1/93.1 kB\u001B[0m \u001B[31m12.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr) (5.9.3)\nInstalling collected packages: brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\nSuccessfully installed brotli-1.1.0 inflate64-1.0.0 multivolumefile-0.2.3 py7zr-0.20.6 pybcj-1.0.2 pycryptodomex-3.19.0 pyppmd-1.0.0 pyzstd-0.15.9\nCollecting sacrebleu\n  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m118.9/118.9 kB\u001B[0m \u001B[31m7.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.6.3)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.23.5)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.3)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.8.2 sacrebleu-2.3.1\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=52c29a4cb3e1a9302fac21eff5645e5af9d520663ccaa83e73221706108fe2ab\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tqdm\n",
    "\n",
    "tqdm.tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T11:10:31.059661700Z",
     "start_time": "2023-11-04T11:10:30.560088700Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-05T15:06:19.043268Z",
     "iopub.execute_input": "2023-11-05T15:06:19.043528Z",
     "iopub.status.idle": "2023-11-05T15:06:19.387459Z",
     "shell.execute_reply.started": "2023-11-05T15:06:19.043505Z",
     "shell.execute_reply": "2023-11-05T15:06:19.386702Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_length = 512"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T11:10:34.008443700Z",
     "start_time": "2023-11-04T11:10:31.062208900Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-05T15:06:19.388497Z",
     "iopub.execute_input": "2023-11-05T15:06:19.388854Z",
     "iopub.status.idle": "2023-11-05T15:06:31.883445Z",
     "shell.execute_reply.started": "2023-11-05T15:06:19.388830Z",
     "shell.execute_reply": "2023-11-05T15:06:31.882664Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading model and its tokenizer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"google/pegasus-xsum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.config.decoder_start_token_id = tokenizer.get_vocab()[\"<n>\"]"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T11:10:39.642531100Z",
     "start_time": "2023-11-04T11:10:34.010445500Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-05T15:06:31.885463Z",
     "iopub.execute_input": "2023-11-05T15:06:31.886074Z",
     "iopub.status.idle": "2023-11-05T15:07:49.943098Z",
     "shell.execute_reply.started": "2023-11-05T15:06:31.886047Z",
     "shell.execute_reply": "2023-11-05T15:07:49.942321Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2a2984441a0426c9b78fc985e783ed9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67cf87f4ecbf43e3b23c65e1ac166a10"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "640f21ec1b7e4e9aa041ab4a3cec3b73"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab1e27fbff8843198efe3bc1d6063586"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "145cbf54553e4508a573110395c77f99"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b29e0183aee47f19a69303295222252"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)neration_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8c1e21bde624ba3baccfe088af3614e"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset('samsum')\n",
    "dataset"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T11:10:41.941343300Z",
     "start_time": "2023-11-04T11:10:39.642531100Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-05T15:07:49.944267Z",
     "iopub.execute_input": "2023-11-05T15:07:49.944550Z",
     "iopub.status.idle": "2023-11-05T15:08:02.633064Z",
     "shell.execute_reply.started": "2023-11-05T15:07:49.944527Z",
     "shell.execute_reply": "2023-11-05T15:08:02.632187Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/1.42k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5fa47e8204884b248be9e740d0d2de99"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading metadata:   0%|          | 0.00/770 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a4dcb441ae648dcb81b26e6d1ba4fe2"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Downloading and preparing dataset samsum/samsum (download: 2.81 MiB, generated: 10.04 MiB, post-processed: Unknown size, total: 12.85 MiB) to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6...\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5078bebb65294b198c656561e2b70f03"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": ""
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": ""
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": ""
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Dataset samsum downloaded and prepared to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6. Subsequent calls will reuse this data.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "391bdb85347a492e8dacf87825d69b01"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 5,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "Preprocess dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=64, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=['id', 'dialogue', 'summary']\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T11:10:41.992667300Z",
     "start_time": "2023-11-04T11:10:41.947344300Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-05T15:08:02.634250Z",
     "iopub.execute_input": "2023-11-05T15:08:02.634529Z",
     "iopub.status.idle": "2023-11-05T15:08:07.408055Z",
     "shell.execute_reply.started": "2023-11-05T15:08:02.634504Z",
     "shell.execute_reply": "2023-11-05T15:08:07.407180Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/15 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19d08dc08e274e15b336210aa2bcf7f3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7c05a5dd67e48c7b6e53b066070439e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ed2ff12c2f7487f9793880d21b9fa72"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "columns = ['input_ids', 'labels', 'attention_mask']\n",
    "tokenized_dataset.set_format(type='torch', columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T11:10:43.888718100Z",
     "start_time": "2023-11-04T11:10:43.884206200Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-05T15:08:14.620385Z",
     "iopub.execute_input": "2023-11-05T15:08:14.620956Z",
     "iopub.status.idle": "2023-11-05T15:08:14.627628Z",
     "shell.execute_reply.started": "2023-11-05T15:08:14.620925Z",
     "shell.execute_reply": "2023-11-05T15:08:14.626668Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Removing some validation data to make evaluation process faster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "execution_count": 7,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'train': (14732, 3), 'test': (819, 3), 'validation': (205, 3)}"
     },
     "metadata": {}
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.shuffle()\n",
    "tokenized_dataset['validation'] = tokenized_dataset['validation'].shard(num_shards=4, index=0)\n",
    "tokenized_dataset.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T11:10:41.980670600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metrics\n",
    "Metric function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "176d7d06c50a42b3a5c8999f0bc8b318"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "107655cf071e47aa91b99e250f41ea24"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8de7e971c16d4249b5cff7b28658f3b4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd290965a8034e5ab50e95fe18037f92"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "blue = evaluate.load('bleu')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    blue_score = blue.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()} | {'bleu': blue_score['bleu']}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T11:10:42.026830100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T11:10:43.894448400Z",
     "start_time": "2023-11-04T11:10:43.886721300Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-05T15:08:14.631119Z",
     "iopub.execute_input": "2023-11-05T15:08:14.631385Z",
     "iopub.status.idle": "2023-11-05T15:08:14.640441Z",
     "shell.execute_reply.started": "2023-11-05T15:08:14.631362Z",
     "shell.execute_reply": "2023-11-05T15:08:14.639447Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize optimizer and scheduler. I use the same scheduler as in paper \"Attention is all you need\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class Scheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer: Optimizer, dim_embed: int, warmup_steps: int, last_epoch: int=-1, verbose: bool=False) -> None:\n",
    "        self.dim_embed = dim_embed\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_param_groups = len(optimizer.param_groups)\n",
    "        super().__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self) -> float:\n",
    "        lr = calc_lr(self._step_count, self.dim_embed, self.warmup_steps)\n",
    "        return [lr] * self.num_param_groups\n",
    "\n",
    "def calc_lr(step, dim_embed, warmup_steps):\n",
    "    return dim_embed**(-0.5) * min(step**(-0.5), step * warmup_steps**(-1.5))\n",
    "\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-9, betas=(0.9, 0.98))\n",
    "\n",
    "train_batch_size = 4\n",
    "val_batch_size = 1\n",
    "num_epochs = 6\n",
    "\n",
    "scheduler = Scheduler(\n",
    "    optimizer,\n",
    "    warmup_steps=0.1 * num_epochs * (tokenized_dataset['train'].num_rows / train_batch_size),\n",
    "    dim_embed=1024\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T11:10:43.901029600Z",
     "start_time": "2023-11-04T11:10:43.893447500Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-05T15:08:14.641427Z",
     "iopub.execute_input": "2023-11-05T15:08:14.641677Z",
     "iopub.status.idle": "2023-11-05T15:08:14.656134Z",
     "shell.execute_reply.started": "2023-11-05T15:08:14.641654Z",
     "shell.execute_reply": "2023-11-05T15:08:14.655381Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = GenerationConfig(\n",
    "    bos_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T11:10:43.934114100Z",
     "start_time": "2023-11-04T11:10:43.903532900Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-05T15:08:14.657283Z",
     "iopub.execute_input": "2023-11-05T15:08:14.657956Z",
     "iopub.status.idle": "2023-11-05T15:08:14.668000Z",
     "shell.execute_reply.started": "2023-11-05T15:08:14.657924Z",
     "shell.execute_reply": "2023-11-05T15:08:14.667175Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setup training arguments and trainer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=val_batch_size,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    num_train_epochs=num_epochs,\n",
    "\n",
    "    predict_with_generate=True,\n",
    "\n",
    "    disable_tqdm=False,\n",
    "    generation_config=gen_config,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T11:10:43.942113900Z",
     "start_time": "2023-11-04T11:10:43.906542400Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-05T15:08:14.669020Z",
     "iopub.execute_input": "2023-11-05T15:08:14.669338Z",
     "iopub.status.idle": "2023-11-05T15:08:19.392084Z",
     "shell.execute_reply.started": "2023-11-05T15:08:14.669308Z",
     "shell.execute_reply": "2023-11-05T15:08:19.391281Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fine-tune the model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T11:33:20.277350700Z",
     "start_time": "2023-11-04T11:27:42.249336700Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-05T15:08:19.393204Z",
     "iopub.execute_input": "2023-11-05T15:08:19.393486Z",
     "iopub.status.idle": "2023-11-05T17:04:16.029377Z",
     "shell.execute_reply.started": "2023-11-05T15:08:19.393463Z",
     "shell.execute_reply": "2023-11-05T17:04:16.027630Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
     "output_type": "stream"
    },
    {
     "output_type": "stream",
     "name": "stdin",
     "text": "  ········································\n"
    },
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.9"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20231105_151331-xb9a21k8</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/birdwithdreams/huggingface/runs/xb9a21k8' target=\"_blank\">devout-frog-59</a></strong> to <a href='https://wandb.ai/birdwithdreams/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/birdwithdreams/huggingface' target=\"_blank\">https://wandb.ai/birdwithdreams/huggingface</a>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/birdwithdreams/huggingface/runs/xb9a21k8' target=\"_blank\">https://wandb.ai/birdwithdreams/huggingface/runs/xb9a21k8</a>"
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='11186' max='22098' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11186/22098 1:50:08 < 1:47:28, 1.69 it/s, Epoch 3.04/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n      <th>Bleu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.976400</td>\n      <td>1.726112</td>\n      <td>0.493100</td>\n      <td>0.242700</td>\n      <td>0.408500</td>\n      <td>0.407000</td>\n      <td>64.000000</td>\n      <td>0.166814</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.613600</td>\n      <td>1.644370</td>\n      <td>0.486600</td>\n      <td>0.240000</td>\n      <td>0.395100</td>\n      <td>0.394300</td>\n      <td>64.000000</td>\n      <td>0.157319</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.265900</td>\n      <td>1.658141</td>\n      <td>0.487700</td>\n      <td>0.249800</td>\n      <td>0.402000</td>\n      <td>0.401900</td>\n      <td>64.000000</td>\n      <td>0.176767</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Start training\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1553\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1554\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1555\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1556\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1557\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1558\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1847\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1844\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1845\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss_step\n\u001B[0;32m-> 1847\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_flos \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloating_point_ops\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1849\u001B[0m is_last_step_and_steps_less_than_grad_acc \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1850\u001B[0m     steps_in_epoch \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m args\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps \u001B[38;5;129;01mand\u001B[39;00m (step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m==\u001B[39m steps_in_epoch\n\u001B[1;32m   1851\u001B[0m )\n\u001B[1;32m   1853\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1854\u001B[0m     total_batched_samples \u001B[38;5;241m%\u001B[39m args\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   1855\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1859\u001B[0m     \u001B[38;5;66;03m# the `or` condition of `is_last_step_and_steps_less_than_grad_acc` is not covered\u001B[39;00m\n\u001B[1;32m   1860\u001B[0m     \u001B[38;5;66;03m# in accelerate. So, explicitly enable sync gradients to True in that case.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3418\u001B[0m, in \u001B[0;36mTrainer.floating_point_ops\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m   3405\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3406\u001B[0m \u001B[38;5;124;03mFor models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\u001B[39;00m\n\u001B[1;32m   3407\u001B[0m \u001B[38;5;124;03moperations for every backward + forward pass. If using another model, either implement such a method in the\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3415\u001B[0m \u001B[38;5;124;03m    `int`: The number of floating-point operations.\u001B[39;00m\n\u001B[1;32m   3416\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3417\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloating_point_ops\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 3418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloating_point_ops\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3419\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3420\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1049\u001B[0m, in \u001B[0;36mModuleUtilsMixin.floating_point_ops\u001B[0;34m(self, input_dict, exclude_embeddings)\u001B[0m\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfloating_point_ops\u001B[39m(\n\u001B[1;32m   1026\u001B[0m     \u001B[38;5;28mself\u001B[39m, input_dict: Dict[\u001B[38;5;28mstr\u001B[39m, Union[torch\u001B[38;5;241m.\u001B[39mTensor, Any]], exclude_embeddings: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1027\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[1;32m   1028\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;124;03m    Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\u001B[39;00m\n\u001B[1;32m   1030\u001B[0m \u001B[38;5;124;03m    batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1046\u001B[0m \u001B[38;5;124;03m        `int`: The number of floating-point operations.\u001B[39;00m\n\u001B[1;32m   1047\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1049\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m6\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimate_tokens(input_dict) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_parameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexclude_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude_embeddings\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:997\u001B[0m, in \u001B[0;36mModuleUtilsMixin.num_parameters\u001B[0;34m(self, only_trainable, exclude_embeddings)\u001B[0m\n\u001B[1;32m    993\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m exclude_embeddings:\n\u001B[1;32m    994\u001B[0m     embedding_param_names \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    995\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.weight\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m name, module_type \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnamed_modules() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(module_type, nn\u001B[38;5;241m.\u001B[39mEmbedding)\n\u001B[1;32m    996\u001B[0m     ]\n\u001B[0;32m--> 997\u001B[0m     non_embedding_parameters \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    998\u001B[0m         parameter \u001B[38;5;28;01mfor\u001B[39;00m name, parameter \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnamed_parameters() \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m embedding_param_names\n\u001B[1;32m    999\u001B[0m     ]\n\u001B[1;32m   1000\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msum\u001B[39m(p\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m non_embedding_parameters \u001B[38;5;28;01mif\u001B[39;00m p\u001B[38;5;241m.\u001B[39mrequires_grad \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m only_trainable)\n\u001B[1;32m   1001\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:997\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    993\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m exclude_embeddings:\n\u001B[1;32m    994\u001B[0m     embedding_param_names \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    995\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.weight\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m name, module_type \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnamed_modules() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(module_type, nn\u001B[38;5;241m.\u001B[39mEmbedding)\n\u001B[1;32m    996\u001B[0m     ]\n\u001B[0;32m--> 997\u001B[0m     non_embedding_parameters \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    998\u001B[0m         parameter \u001B[38;5;28;01mfor\u001B[39;00m name, parameter \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnamed_parameters() \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m embedding_param_names\n\u001B[1;32m    999\u001B[0m     ]\n\u001B[1;32m   1000\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msum\u001B[39m(p\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m non_embedding_parameters \u001B[38;5;28;01mif\u001B[39;00m p\u001B[38;5;241m.\u001B[39mrequires_grad \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m only_trainable)\n\u001B[1;32m   1001\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2115\u001B[0m, in \u001B[0;36mModule.named_parameters\u001B[0;34m(self, prefix, recurse, remove_duplicate)\u001B[0m\n\u001B[1;32m   2090\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Returns an iterator over module parameters, yielding both the\u001B[39;00m\n\u001B[1;32m   2091\u001B[0m \u001B[38;5;124;03mname of the parameter as well as the parameter itself.\u001B[39;00m\n\u001B[1;32m   2092\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2110\u001B[0m \n\u001B[1;32m   2111\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2112\u001B[0m gen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_named_members(\n\u001B[1;32m   2113\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m module: module\u001B[38;5;241m.\u001B[39m_parameters\u001B[38;5;241m.\u001B[39mitems(),\n\u001B[1;32m   2114\u001B[0m     prefix\u001B[38;5;241m=\u001B[39mprefix, recurse\u001B[38;5;241m=\u001B[39mrecurse, remove_duplicate\u001B[38;5;241m=\u001B[39mremove_duplicate)\n\u001B[0;32m-> 2115\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m gen\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2049\u001B[0m, in \u001B[0;36mModule._named_members\u001B[0;34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001B[0m\n\u001B[1;32m   2047\u001B[0m memo \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[1;32m   2048\u001B[0m modules \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnamed_modules(prefix\u001B[38;5;241m=\u001B[39mprefix, remove_duplicate\u001B[38;5;241m=\u001B[39mremove_duplicate) \u001B[38;5;28;01mif\u001B[39;00m recurse \u001B[38;5;28;01melse\u001B[39;00m [(prefix, \u001B[38;5;28mself\u001B[39m)]\n\u001B[0;32m-> 2049\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m module_prefix, module \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   2050\u001B[0m     members \u001B[38;5;241m=\u001B[39m get_members_fn(module)\n\u001B[1;32m   2051\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m members:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2266\u001B[0m, in \u001B[0;36mModule.named_modules\u001B[0;34m(self, memo, prefix, remove_duplicate)\u001B[0m\n\u001B[1;32m   2264\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m   2265\u001B[0m submodule_prefix \u001B[38;5;241m=\u001B[39m prefix \u001B[38;5;241m+\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m prefix \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m name\n\u001B[0;32m-> 2266\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001B[1;32m   2267\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2266\u001B[0m, in \u001B[0;36mModule.named_modules\u001B[0;34m(self, memo, prefix, remove_duplicate)\u001B[0m\n\u001B[1;32m   2264\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m   2265\u001B[0m submodule_prefix \u001B[38;5;241m=\u001B[39m prefix \u001B[38;5;241m+\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m prefix \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m name\n\u001B[0;32m-> 2266\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001B[1;32m   2267\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m m\n",
      "    \u001B[0;31m[... skipping similar frames: Module.named_modules at line 2266 (3 times)]\u001B[0m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2266\u001B[0m, in \u001B[0;36mModule.named_modules\u001B[0;34m(self, memo, prefix, remove_duplicate)\u001B[0m\n\u001B[1;32m   2264\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m   2265\u001B[0m submodule_prefix \u001B[38;5;241m=\u001B[39m prefix \u001B[38;5;241m+\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m prefix \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m name\n\u001B[0;32m-> 2266\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnamed_modules\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmemo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubmodule_prefix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremove_duplicate\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   2267\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/working/output/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-05T17:04:30.588128Z",
     "iopub.execute_input": "2023-11-05T17:04:30.588494Z",
     "iopub.status.idle": "2023-11-05T17:04:30.600592Z",
     "shell.execute_reply.started": "2023-11-05T17:04:30.588465Z",
     "shell.execute_reply": "2023-11-05T17:04:30.599566Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/working/output/checkpoint-3683/scheduler.pt\n/kaggle/working/output/checkpoint-3683/optimizer.pt\n/kaggle/working/output/checkpoint-3683/trainer_state.json\n/kaggle/working/output/checkpoint-3683/rng_state.pth\n/kaggle/working/output/checkpoint-3683/pytorch_model.bin\n/kaggle/working/output/checkpoint-3683/training_args.bin\n/kaggle/working/output/checkpoint-3683/config.json\n/kaggle/working/output/checkpoint-3683/generation_config.json\n/kaggle/working/output/runs/Nov05_15-08-14_420d3b5a0eaa/events.out.tfevents.1699196899.420d3b5a0eaa.32.0\n/kaggle/working/output/checkpoint-7366/scheduler.pt\n/kaggle/working/output/checkpoint-7366/optimizer.pt\n/kaggle/working/output/checkpoint-7366/trainer_state.json\n/kaggle/working/output/checkpoint-7366/rng_state.pth\n/kaggle/working/output/checkpoint-7366/pytorch_model.bin\n/kaggle/working/output/checkpoint-7366/training_args.bin\n/kaggle/working/output/checkpoint-7366/config.json\n/kaggle/working/output/checkpoint-7366/generation_config.json\n/kaggle/working/output/checkpoint-11049/scheduler.pt\n/kaggle/working/output/checkpoint-11049/optimizer.pt\n/kaggle/working/output/checkpoint-11049/trainer_state.json\n/kaggle/working/output/checkpoint-11049/rng_state.pth\n/kaggle/working/output/checkpoint-11049/pytorch_model.bin\n/kaggle/working/output/checkpoint-11049/training_args.bin\n/kaggle/working/output/checkpoint-11049/config.json\n/kaggle/working/output/checkpoint-11049/generation_config.json\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline('summarization', model='/kaggle/working/output/checkpoint-11049', tokenizer=tokenizer)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-05T17:21:46.089425Z",
     "iopub.execute_input": "2023-11-05T17:21:46.089960Z",
     "iopub.status.idle": "2023-11-05T17:21:55.942976Z",
     "shell.execute_reply.started": "2023-11-05T17:21:46.089915Z",
     "shell.execute_reply": "2023-11-05T17:21:55.941404Z"
    },
    "trusted": true
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text = tokenized_dataset['test']['input_ids'][:10]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-05T17:21:55.953180Z",
     "iopub.execute_input": "2023-11-05T17:21:55.953771Z",
     "iopub.status.idle": "2023-11-05T17:21:56.004305Z",
     "shell.execute_reply.started": "2023-11-05T17:21:55.953733Z",
     "shell.execute_reply": "2023-11-05T17:21:56.002997Z"
    },
    "trusted": true
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prediction:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pipe(tokenizer.batch_decode(text))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-05T17:21:56.006842Z",
     "iopub.execute_input": "2023-11-05T17:21:56.007212Z",
     "iopub.status.idle": "2023-11-05T17:22:48.504866Z",
     "shell.execute_reply.started": "2023-11-05T17:21:56.007184Z",
     "shell.execute_reply": "2023-11-05T17:22:48.503852Z"
    },
    "trusted": true
   },
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "text": "Your max_length is set to 64, but your input_length is only 54. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\nYour max_length is set to 64, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
     "output_type": "stream"
    },
    {
     "execution_count": 47,
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'summary_text': \"<n> Mazie and her friends are having a drink tonight. Lee can't stand some of her friends.\"},\n {'summary_text': \"<n> There was a weird smell at Kasia's place last night. Clara suspects it was her boyfriend. Ron doesn't know what it was.\"},\n {'summary_text': \"<n> The readings for the next session of Stephen's seminar are not in the print shop yet. Rita will go to the shop tomorrow morning and let them know if the texts are available.\"},\n {'summary_text': \"<n> Marleen will be in half an hour to go to town. Barry has no idea what to buy Rita for Christmas. Marleen is at Kaiser's. Marleen and Barry will go for 50/50.\"},\n {'summary_text': '<n> Sam started the career mode in 18 months.'},\n {'summary_text': '<n> There was a big media fuss about a meet and greet with James Charles in Birmingham. There were 8000 people in the mall. A host from LBC tried to answer the question \"Who is James Charles\".'},\n {'summary_text': '<n> Patricia sent a file to Elle and Florence about Fair-Trade brand.'},\n {'summary_text': \"<n> Eva is going to visit her mother, but she doesn't want to be at Olivia's party. Olivia is having fun with her grand daughter.\"},\n {'summary_text': \"<n> Stanley is alive and well. everybody's discussing Stan Lee.\"},\n {'summary_text': '<n> Mike had an accident on his motorcycle and broke his leg.'}]"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Target:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer.batch_decode(tokenized_dataset['test']['labels'][:10])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-05T17:22:48.506205Z",
     "iopub.execute_input": "2023-11-05T17:22:48.506553Z",
     "iopub.status.idle": "2023-11-05T17:22:48.549629Z",
     "shell.execute_reply.started": "2023-11-05T17:22:48.506518Z",
     "shell.execute_reply": "2023-11-05T17:22:48.548656Z"
    },
    "trusted": true
   },
   "execution_count": 48,
   "outputs": [
    {
     "execution_count": 48,
     "output_type": "execute_result",
     "data": {
      "text/plain": "[\"Mazie and her girlfriends are having a drink tonight. Lee doesn't like some of her friends, for example Sheryl. Lee had spilt a drink on her.</s>\",\n \"Clara and Ron are wondering what that weird smell at Kasia's place last night was.</s>\",\n \"Chae-yeong and Arthur inform Mariana that the readings for the next session of Stephen's seminar weren't available in the print shop on Monday. Rita decides to go and chcek if the situation's changed tomorrow and she'll let everyone know.</s>\",\n \"Marleen will be leaving in half an hour to go to town. She will get a present for Rita - silk kerchief with a yellow pattern from Kaiser's. It costs 39, and Barley will share the cost 50/50 with Marleen.</s>\",\n 'Sam started a career mode in FIFA.</s>',\n 'There was a meet-and-greet with James Charles in Birmingham which gathered 8000 people.</s>',\n 'Patricia is recommending a fair-trade brand to Elle and Florence.</s>',\n 'Eva is at a party, while Olivia is taking care of her daughter, Linta. Eva will leave soon and pick Linta up.</s>',\n 'Stan Lee is dead.</s>',\n \"Mike's had an accident on his motorcycle and he's broken his leg.</s>\"]"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dialogue:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer.batch_decode(text)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-05T17:22:48.550879Z",
     "iopub.execute_input": "2023-11-05T17:22:48.551265Z",
     "iopub.status.idle": "2023-11-05T17:22:48.561567Z",
     "shell.execute_reply.started": "2023-11-05T17:22:48.551229Z",
     "shell.execute_reply": "2023-11-05T17:22:48.560429Z"
    },
    "trusted": true
   },
   "execution_count": 49,
   "outputs": [
    {
     "execution_count": 49,
     "output_type": "execute_result",
     "data": {
      "text/plain": "[\"summarize: Mazie: Me and mah girls are having a drink tonight. Lee: Oh really? Where? Mazie: Wouldn't you like to know... Lee: Yes! So I can stay away! Mazie: That's cold. Lee: True, can't stand some of your friends. Mazie: OIC...who? Lee: Sheryl for one. She's a b-word that rhymes with witch. Mazie: You just saw her on a bad day. Lee: Don't think so! Mazie: You did spill a drink on her. Lee: Well... Mazie: You did! Lee: I said sorry! Mazie: LOL</s>\",\n \"summarize: Clara: Did you notice that weird smell at Kasia's place last night? Ron: YES!!!!!! I didn't want to say anything about it, though. I didn't want to be rude. Clara: I think it was her 21 cats roaming around lol Ron: lol don't say that, those cats were cute. Clara: so what? they can still smell Ron: i think it was her sleazy boyfriend Clara: lol you're bad Ron: jk Clara: in all honesty I don't know what it was. Ron: i guess we'll never know</s>\",\n 'summarize: Mariana: Hi, just a quick question. Do you know if the readings for the next session of Stephen’s seminar are in the print shop? Rita: No idea, sorry Chae-yeong: The only thing I know is that they were not yet there on Monday Arthur: Yer, I’ve made the mistake of going on Monday as well and I can confirm the texts were not there and the staff was as rude as always Mariana: Sounds familiar Rita: I’ll go tomorrow morning and let you know if the texts are available Rita: I can’t buy the copies for you because they never have enough at hand but I’ll just ask them to print more for later Chae-yeong: No worries Arthur: Yeah, just let us know if they’re ready, that’s a huge favour already Rita: Cool. Will do! xx</s>',\n \"summarize: Marleen: Hello my dear, I'll be leaving in half an hour to go to town. If you have any idea for a present for Rita let me know. Marleen: Text me here with your ideas for a present for her. Barry: But I've got none! I leave the choice entirely to you. Barry: I'm at home now. Marleen: Am at Kaiser's. Lovely silk kerchiefs. What do you think? Barry: One can't have too many! Marleen: But isn't it a bit too personal? Barry: Not at all. Marleen: <unk>file_photo> Marleen: Which? Barry: The one with yellow pattern, definitely. She loves yellow. Marleen: The most expensive of the five. Barry: How much? Marleen: 39 Barry: Oh come on. Not so bad. Marleen: Shall we go 50/50? Barry: OK</s>\",\n 'summarize: Sam: so i started the career mode in fifa Tim: 19? Sam: yea Sam: its so different from the one in 18 maaan Tim: yeah? Tim: whats different Sam: alot of things Sam: champions league for instance Sam: CHAMPIONS LEAGUE BABYY XD Tim: cool xD</s>',\n 'summarize: Cora: Have you heard how much fuss British media made about meet and greet with James Charles in Birmingham? Ellie: no...! what happened? Cora: Well, there was a meet and greet with James Charles in one of the malls in Birmingham and about 8000 fans showed up for it. Cora: It cause a gridlock around the mall and - of course - British media had to make some (quite negative) comments on it. Ellie: they came for sister James?! >:( Ellie: i sister snapped!! :p :D Cora: Haha :D Cora: You shouldn\\'t watch so much youtube, you\\'re getting weirder and weirder. :d Ellie: sister shut up :P so, what did they say? Cora: ;) :* \"Daily Mail\" was surprised that a meet and greet with a \"virtually unknown\" youtuber gathered 8000 people. :p Cora: A host from LBC tried to find an answer to an unanswerable question: \"Who is James Charles?\". Eventually James called him and introduced himself. On air. :D Ellie: there\\'s something called google lol Cora: Right? :p Cora: Some hosts from ITV Central couldn\\'t wrap their heads around the fact that a guy can wear makeup. Ellie: really??? Ellie: smh it\\'s 21st century, they should have noticed already... there are so many amazing male makeup artists Cora: I agree! There are still plenty of dinosaurs in the media. :/</s>',\n \"summarize: Patricia: Hello, here's the fair-trade brand I've been talking about <unk>file_other> Elle: Oh, thanks! Florence: Looks great! Patricia: I'm glad, I hope you enjoy it. The quality's really great and knowing where it came from makes it easier to spend the extra dollar ;) Elle: I'll look into it :) Florence: Thx</s>\",\n 'summarize: Eva: hi mom.. hows linta? Olivia: hi honey... she is good Eva: hope she is not bothering you? Olivia: no dear we are enjoying each others company... Eva: reallly i am so glad! Olivia: yes my dear dont worry and enjoy your party... Eva: thank you mom.. i would be leaving in an hour Olivia: oh no take your time i am having fun with my grand daughter Eva: no mom i have to go home and every one would be leaving too. Olivia: ok then leave her with me for a day i will drop her tomorrow Eva: no mom Jones loves Linta he wont be able to sleep without playing with out her Olivia: awww ok :( i will keep her ready and her bag too.. do u want me to make a bottle of milk also Eva: yes mom please Olivia: ok darling Eva: love you mom :kisses:</s>',\n \"summarize: Jack: OMG STANLEY IS DEAD Oliver: Wtf? Stanley: I'm alive and well, dude Jack: I meant Stan Lee, damn autocorrect Stanley: I guessed that XD Yep, everybody's posting about it right now :(</s>\",\n 'summarize: Ian: Did you hear? Kate: What happened? Ian: Mike had an accident on his motorcycle. Ian: He broke his leg</s>']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conclusion: \n",
    "We evaluate our model using BLUE and ROUGE scores. Let's describe their pros and cons in general:\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) Score:\n",
    "- Pros:\n",
    "  - It's a fast and cost-effective way to measure the quality of Machine Translation output.\n",
    "  - It's language-independent and correlates positively with human evaluation.\n",
    "- Cons:\n",
    "  - BLEU score measures syntactical matches rather than semantics.\n",
    "  - It doesn't manage different words that have the same meaning.\n",
    "  - It's not a percentage measure of accuracy.\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) Score:\n",
    "- Pros:\n",
    "  - It correlates positively with human evaluation, it’s inexpensive to compute and language-independent.\n",
    "  - It measures the longest matching sequence of words using LCS (Longest Common Subsequence), which does not require consecutive matches but in-sequence matches that reflect sentence-level word order.\n",
    "- Cons:\n",
    "  - ROUGE does not manage different words that have the same meaning, as it measures syntactical matches rather than semantics.\n",
    "  - It focuses on recall, which means it measures how much the words (and/or n-grams) in the human references appear in the candidate model outputs. This might not be ideal in scenarios where precision is also important.\n",
    "\n",
    "We can clearly see all the pros and cons of these metrics on generated results of our model. \n",
    "BLUE score is low due to the fact that model can't clearly generate grammar and sometimes describe semantic with not exact words, but synonyms.\n",
    "ROUGE score is higher, because there are a lot of cases when model generates exact subsequences as in target. For example:\n",
    "   - Target: **Mazie and her girlfriends are having a drink tonight**. Lee doesn't like **some of her friends**, for example Sheryl. Lee had spilt a drink on her.\n",
    "   - Prediction: **Mazie and her friends are having a drink tonight**. Lee can't **stand some of her friends**.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LoRa attempt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_length = 512\n",
    "\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.config.decoder_start_token_id = tokenizer.get_vocab()[\"<n>\"]\n",
    "\n",
    "dataset = load_dataset('samsum')\n",
    "\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=64, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=['id', 'dialogue', 'summary']\n",
    ")\n",
    "\n",
    "columns = ['input_ids', 'labels', 'attention_mask']\n",
    "tokenized_dataset.set_format(type='torch', columns=columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "blue = evaluate.load('bleu')\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    blue_score = blue.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()} | {'bleu': blue_score['bleu']}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\" \n",
    "    Prints the number of trainable parameters in the model. \n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", 'fc1', 'fc2'],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "train_batch_size = 4\n",
    "val_batch_size = 1\n",
    "num_epochs = 4\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0.1 * num_epochs * (tokenized_dataset['train'].num_rows / train_batch_size),\n",
    "    num_training_steps=num_epochs * (tokenized_dataset['train'].num_rows / train_batch_size)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = GenerationConfig(bos_token_id=model.config.decoder_start_token_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=val_batch_size,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    num_train_epochs=num_epochs,\n",
    "\n",
    "    predict_with_generate=True,\n",
    "\n",
    "    disable_tqdm=False,\n",
    "    generation_config=gen_config,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model was successfully trained, but I can't find the way how load it from checkpoint and make predictions on test data. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
