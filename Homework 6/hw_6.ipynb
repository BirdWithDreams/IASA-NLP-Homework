{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "tqdm.tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T09:45:41.298469Z",
     "start_time": "2023-11-04T09:45:40.908424Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, TextDataset, DataCollatorForSeq2Seq, Seq2SeqTrainer, \\\n",
    "    Seq2SeqTrainingArguments\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_length = 1024"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-02T09:30:50.283820Z",
     "iopub.execute_input": "2023-11-02T09:30:50.284868Z",
     "iopub.status.idle": "2023-11-02T09:31:05.832767Z",
     "shell.execute_reply.started": "2023-11-02T09:30:50.284831Z",
     "shell.execute_reply": "2023-11-02T09:31:05.831851Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-04T09:45:44.990816900Z",
     "start_time": "2023-11-04T09:45:41.300594Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Set your model and tokenizer name\n",
    "model_name = \"gpt2\"  # You can use other variants like \"gpt2-medium\", \"gpt2-large\" etc\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-02T09:33:22.531551Z",
     "iopub.execute_input": "2023-11-02T09:33:22.531965Z",
     "iopub.status.idle": "2023-11-02T09:33:26.694345Z",
     "shell.execute_reply.started": "2023-11-02T09:33:22.531933Z",
     "shell.execute_reply": "2023-11-02T09:33:26.692986Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-04T09:45:46.369042700Z",
     "start_time": "2023-11-04T09:45:44.992817200Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mpad_token \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39meos_token\n\u001B[0;32m      5\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mpad_token_id \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39meos_token_id\n\u001B[1;32m----> 7\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForSeq2SeqLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[0;32m      8\u001B[0m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpad_token_id \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39meos_token_id\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IASA-NLP-Homework\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:568\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    564\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m    565\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    566\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    567\u001B[0m     )\n\u001B[1;32m--> 568\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    571\u001B[0m )\n",
      "\u001B[1;31mValueError\u001B[0m: Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset('samsum')\n",
    "dataset"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-02T09:33:26.696548Z",
     "iopub.execute_input": "2023-11-02T09:33:26.697249Z",
     "iopub.status.idle": "2023-11-02T09:33:27.164385Z",
     "shell.execute_reply.started": "2023-11-02T09:33:26.697213Z",
     "shell.execute_reply": "2023-11-02T09:33:27.163394Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-04T08:40:42.802951400Z",
     "start_time": "2023-11-04T08:40:40.644358300Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "prefix = \"summarize the following document:\\n\\n\"\n",
    "suffix = '\\nTL;DR:\\n'\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    prompt = [doc + suffix for doc in examples['dialogue']]\n",
    "    model_input = tokenizer(\n",
    "        text=prompt, text_target=prompt, text_pair=examples['summary'], text_pair_target=examples['summary'],\n",
    "        max_length=max_length, truncation=True\n",
    "        )\n",
    "\n",
    "    return model_input\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T08:40:42.812421900Z",
     "start_time": "2023-11-04T08:40:42.806100900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cc41464b2e1438fb1c024bda176b2a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/818 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10fe2b1f428646a0b66165afc4d7c2e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(283, 283)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_features(examples):\n",
    "    prompt = [doc + suffix for doc in examples['dialogue']]\n",
    "    model_input = tokenizer(\n",
    "        text=prompt, text_target=examples['summary'], max_length=max_length, truncation=True\n",
    "    )\n",
    "    labels = [[-100] * len(sample[0]) + sample[1] for sample in zip(model_input['input_ids'], model_input['labels'])]\n",
    "    model_input['input_ids'] = [sample[0] + sample[1] for sample in zip(model_input['input_ids'], model_input['labels'])]\n",
    "    model_input['labels'] = labels\n",
    "    model_input['attention_mask'] = [[1] * sample.count(-100) + [0] * (len(sample) - sample.count(-100)) for sample in model_input['labels']]\n",
    "\n",
    "    return model_input\n",
    "tokenized_train_dataset = dataset['train'].map(get_features, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n",
    "tokenized_valid_dataset = dataset['validation'].map(get_features, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n",
    "\n",
    "tokenized_dataset = DatasetDict({\n",
    "    'train': tokenized_train_dataset,\n",
    "    'validation': tokenized_valid_dataset\n",
    "})\n",
    "len(tokenized_dataset['train']['input_ids'][9]), len(tokenized_dataset['train']['labels'][9])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:57:01.314322200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    prompt = [doc + suffix for doc in examples['dialogue']]\n",
    "    model_input = tokenizer(\n",
    "        text=prompt, text_target=examples['summary'], max_length=max_length, truncation=True\n",
    "    )\n",
    "    return model_input"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:55:42.805232500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[-100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n -100,\n 45,\n 3883,\n 338,\n 1762,\n 287,\n 3936,\n 11,\n 475,\n 262,\n 3988,\n 6487,\n 379,\n 607,\n 22945,\n 18702,\n 13,\n 1375,\n 338,\n 2406,\n 1363,\n 287,\n 718,\n 2745,\n 13,\n 20635,\n 621,\n 326,\n 673,\n 338,\n 1016,\n 284,\n 3067,\n 351,\n 513,\n 584,\n 2490,\n 82,\n 13]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tokenized_dataset['validation']['labels'][9])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T08:40:44.736067300Z",
     "start_time": "2023-11-04T08:40:44.685530500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "{'train': (148, 3), 'validation': (818, 3)}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.shuffle()\n",
    "tokenized_dataset['train'] = tokenized_dataset['train'].shard(num_shards=100, index=0)\n",
    "tokenized_dataset.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T08:57:19.014534900Z",
     "start_time": "2023-11-04T08:57:18.983012700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sacrebleu\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions[:, :labels.shape[1]]\n",
    "    predictions = np.where(labels != -100, predictions, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels tensor with tokenizer.pad_token_id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu = sacrebleu.corpus_bleu(decoded_preds, [decoded_labels])\n",
    "\n",
    "    return {\n",
    "        'bleu': bleu.score,\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "columns = ['input_ids', 'labels', 'attention_mask']\n",
    "tokenized_dataset.set_format(type='torch', columns=columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "train_batch_size = 2\n",
    "val_batch_size = 1\n",
    "num_epochs = 10\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0.1 * num_epochs * (tokenized_dataset['train'].num_rows / train_batch_size),\n",
    "    num_training_steps=num_epochs * (tokenized_dataset['train'].num_rows / val_batch_size)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = GenerationConfig(max_new_tokens=20, max_length=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    num_train_epochs=num_epochs,\n",
    "\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    disable_tqdm=False,\n",
    "    generation_config=gen_config,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrii\\anaconda3\\envs\\IASA-NLP-Homework\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/740 : < :, Epoch 0.01/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 73\u001B[0m\n\u001B[0;32m     62\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Seq2SeqTrainer(\n\u001B[0;32m     63\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     64\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     69\u001B[0m     optimizers\u001B[38;5;241m=\u001B[39m(optimizer, scheduler)\n\u001B[0;32m     70\u001B[0m )\n\u001B[0;32m     72\u001B[0m \u001B[38;5;66;03m# Start training\u001B[39;00m\n\u001B[1;32m---> 73\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IASA-NLP-Homework\\Lib\\site-packages\\transformers\\trainer.py:1591\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1589\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   1590\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1591\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[0;32m   1592\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[0;32m   1593\u001B[0m         resume_from_checkpoint\u001B[38;5;241m=\u001B[39mresume_from_checkpoint,\n\u001B[0;32m   1594\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[0;32m   1595\u001B[0m         ignore_keys_for_eval\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n\u001B[0;32m   1596\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IASA-NLP-Homework\\Lib\\site-packages\\transformers\\trainer.py:1999\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1996\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol\u001B[38;5;241m.\u001B[39mshould_training_stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1998\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_epoch_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m-> 1999\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001B[0;32m   2001\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m DebugOption\u001B[38;5;241m.\u001B[39mTPU_METRICS_DEBUG \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdebug:\n\u001B[0;32m   2002\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_torch_tpu_available():\n\u001B[0;32m   2003\u001B[0m         \u001B[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IASA-NLP-Homework\\Lib\\site-packages\\transformers\\trainer.py:2328\u001B[0m, in \u001B[0;36mTrainer._maybe_log_save_evaluate\u001B[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2326\u001B[0m         metrics\u001B[38;5;241m.\u001B[39mupdate(dataset_metrics)\n\u001B[0;32m   2327\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2328\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate(ignore_keys\u001B[38;5;241m=\u001B[39mignore_keys_for_eval)\n\u001B[0;32m   2329\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_report_to_hp_search(trial, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step, metrics)\n\u001B[0;32m   2331\u001B[0m \u001B[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IASA-NLP-Homework\\Lib\\site-packages\\transformers\\trainer_seq2seq.py:165\u001B[0m, in \u001B[0;36mSeq2SeqTrainer.evaluate\u001B[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001B[0m\n\u001B[0;32m    162\u001B[0m     gen_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_beams\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgeneration_num_beams\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gen_kwargs \u001B[38;5;241m=\u001B[39m gen_kwargs\n\u001B[1;32m--> 165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mevaluate(eval_dataset, ignore_keys\u001B[38;5;241m=\u001B[39mignore_keys, metric_key_prefix\u001B[38;5;241m=\u001B[39mmetric_key_prefix)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IASA-NLP-Homework\\Lib\\site-packages\\transformers\\trainer.py:3066\u001B[0m, in \u001B[0;36mTrainer.evaluate\u001B[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[0;32m   3063\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   3065\u001B[0m eval_loop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprediction_loop \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39muse_legacy_prediction_loop \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation_loop\n\u001B[1;32m-> 3066\u001B[0m output \u001B[38;5;241m=\u001B[39m eval_loop(\n\u001B[0;32m   3067\u001B[0m     eval_dataloader,\n\u001B[0;32m   3068\u001B[0m     description\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3069\u001B[0m     \u001B[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001B[39;00m\n\u001B[0;32m   3070\u001B[0m     \u001B[38;5;66;03m# self.args.prediction_loss_only\u001B[39;00m\n\u001B[0;32m   3071\u001B[0m     prediction_loss_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_metrics \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   3072\u001B[0m     ignore_keys\u001B[38;5;241m=\u001B[39mignore_keys,\n\u001B[0;32m   3073\u001B[0m     metric_key_prefix\u001B[38;5;241m=\u001B[39mmetric_key_prefix,\n\u001B[0;32m   3074\u001B[0m )\n\u001B[0;32m   3076\u001B[0m total_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39meval_batch_size \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mworld_size\n\u001B[0;32m   3077\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_key_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_jit_compilation_time\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m output\u001B[38;5;241m.\u001B[39mmetrics:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IASA-NLP-Homework\\Lib\\site-packages\\transformers\\trainer.py:3359\u001B[0m, in \u001B[0;36mTrainer.evaluation_loop\u001B[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[0;32m   3355\u001B[0m         metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_metrics(\n\u001B[0;32m   3356\u001B[0m             EvalPrediction(predictions\u001B[38;5;241m=\u001B[39mall_preds, label_ids\u001B[38;5;241m=\u001B[39mall_labels, inputs\u001B[38;5;241m=\u001B[39mall_inputs)\n\u001B[0;32m   3357\u001B[0m         )\n\u001B[0;32m   3358\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3359\u001B[0m         metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_metrics(EvalPrediction(predictions\u001B[38;5;241m=\u001B[39mall_preds, label_ids\u001B[38;5;241m=\u001B[39mall_labels))\n\u001B[0;32m   3360\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3361\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m {}\n",
      "Cell \u001B[1;32mIn[19], line 18\u001B[0m, in \u001B[0;36mcompute_metrics\u001B[1;34m(eval_pred)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Compute BLEU score\u001B[39;00m\n\u001B[0;32m     15\u001B[0m bleu \u001B[38;5;241m=\u001B[39m sacrebleu\u001B[38;5;241m.\u001B[39mcorpus_bleu(decoded_preds, [decoded_labels])\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m---> 18\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbleu\u001B[39m\u001B[38;5;124m'\u001B[39m: bleu\u001B[38;5;241m.\u001B[39mscore,\n\u001B[0;32m     19\u001B[0m }\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1095\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1053\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.3.1\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[1;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[0;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[1;32m--> 169\u001B[0m         main_debugger\u001B[38;5;241m.\u001B[39mdo_wait_suspend(thread, frame, event, arg)\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.3.1\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread)\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.3.1\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Start training\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:57:22.578851300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline('text-generation', model='gpt2-medium')"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-04T08:40:44.771594900Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = prefix + dataset['train']['dialogue'][1] + suffix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:40:44.773595400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = dataset['train']['dialogue'][1] + \"\\nTL;DR:\\n\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:40:44.774596200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipe(text, clean_up_tokenization_spaces=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:40:44.775596400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:40:44.776596700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-04T08:40:44.777597400Z"
    }
   }
  }
 ]
}
