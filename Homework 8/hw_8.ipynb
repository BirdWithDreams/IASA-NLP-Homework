{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:03:43.426918700Z",
     "start_time": "2023-11-26T20:03:40.218709900Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "from optimum.pipelines import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fe351db24b4bc82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly, we load data to test our models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a602a9c8699dd762"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('samsum')\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:03:46.736567700Z",
     "start_time": "2023-11-26T20:03:43.429388Z"
    }
   },
   "id": "7293ba70f69bd0c6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 103\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'].shuffle()\n",
    "dataset['validation'] = dataset['validation'].shard(8, 0)\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:03:46.745702300Z",
     "start_time": "2023-11-26T20:03:46.728055600Z"
    }
   },
   "id": "56b075fd8d94cd07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4db2679155dbdc72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we have functions to measure the running time of model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88ef24507b8c633b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import time\n",
    "def model_time_evaluation(model, tokenizer, data):\n",
    "    start_time = time.perf_counter()\n",
    "    for text in data:\n",
    "        inputs = tokenizer(text=text['dialogue'], text_target=text['summary'], return_tensors=\"pt\", max_length=512, padding='max_length',\n",
    "                           truncation=True)\n",
    "        inputs['decoder_input_ids'] = inputs['labels']\n",
    "        del inputs['labels']\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "    end_time = time.perf_counter()\n",
    "    return end_time - start_time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:03:46.777734500Z",
     "start_time": "2023-11-26T20:03:46.747703200Z"
    }
   },
   "id": "8864f104dffbd058"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple ONNX"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "598be760c3d039d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Converting `google/pegasus-xsum` transformer to onnx format using `optimum-cli` command and save it to `./model/pegasus/`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c41982a53410647"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The task `text2text-generation` was manually specified, and past key values will not be reused in the decoding. if needed, please pass `--task text2text-generation-with-past` to export using the past key values.\n",
      "Using the export variant default. Available variants are:\n",
      "\t- default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.1.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "C:\\Users\\Andrii\\anaconda3\\envs\\IASA_NLP\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:213: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "C:\\Users\\Andrii\\anaconda3\\envs\\IASA_NLP\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:220: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "C:\\Users\\Andrii\\anaconda3\\envs\\IASA_NLP\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:252: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "Using framework PyTorch: 2.1.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "C:\\Users\\Andrii\\anaconda3\\envs\\IASA_NLP\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:66: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "C:\\Users\\Andrii\\anaconda3\\envs\\IASA_NLP\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "Post-processing the exported models...\n",
      "Deduplicating shared (tied) weights...\n",
      "Could not find ONNX initializer for torch parameter model.decoder.embed_tokens.weight. model.decoder.embed_tokens.weight will not be checked for deduplication.\n",
      "Could not find ONNX initializer for torch parameter model.encoder.embed_tokens.weight. model.encoder.embed_tokens.weight will not be checked for deduplication.\n",
      "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
      "\tlm_head.weight: {'onnx::MatMul_5077'}\n",
      "\tmodel.decoder.embed_tokens.weight: set() --> ignored (may be a parameter from a part of the model not exported)\n",
      "\tmodel.encoder.embed_tokens.weight: set() --> ignored (may be a parameter from a part of the model not exported)\n",
      "\tmodel.shared.weight: {'model.shared.weight'}\n",
      "Removing duplicate initializer onnx::MatMul_5077...\n",
      "Validating models in subprocesses...\n",
      "Validating ONNX model model/pegasus/encoder_model.onnx...\n",
      "\t-[\\u2713] ONNX model output names match reference model (last_hidden_state)\n",
      "\t- Validating ONNX Model output \"last_hidden_state\":\n",
      "\t\t-[\\u2713] (2, 16, 1024) matches (2, 16, 1024)\n",
      "\t\t-[x] values not close enough, max diff: 0.00022137165069580078 (atol: 1e-05)\n",
      "Validating ONNX model model/pegasus/decoder_model.onnx...\n",
      "\t-[\\u2713] ONNX model output names match reference model (present.2.encoder.key, present.5.encoder.value, present.7.decoder.value, present.4.encoder.key, present.11.decoder.key, present.11.encoder.value, present.0.encoder.key, present.3.decoder.key, present.13.decoder.value, present.14.decoder.value, present.7.decoder.key, present.4.encoder.value, present.1.encoder.key, present.2.decoder.key, present.15.encoder.key, present.2.encoder.value, present.10.encoder.key, present.15.decoder.value, present.0.decoder.value, present.8.decoder.value, present.11.decoder.value, present.14.encoder.key, present.15.decoder.key, present.12.encoder.value, logits, present.9.encoder.key, present.10.decoder.key, present.6.decoder.key, present.6.encoder.key, present.7.encoder.value, present.12.decoder.key, present.6.decoder.value, present.13.encoder.value, present.9.decoder.value, present.4.decoder.value, present.5.encoder.key, present.4.decoder.key, present.10.encoder.value, present.11.encoder.key, present.12.decoder.value, present.0.encoder.value, present.1.decoder.value, present.5.decoder.key, present.5.decoder.value, present.8.encoder.value, present.7.encoder.key, present.0.decoder.key, present.1.decoder.key, present.6.encoder.value, present.13.encoder.key, present.15.encoder.value, present.8.encoder.key, present.14.encoder.value, present.9.encoder.value, present.8.decoder.key, present.3.encoder.key, present.10.decoder.value, present.3.encoder.value, present.13.decoder.key, present.14.decoder.key, present.9.decoder.key, present.3.decoder.value, present.1.encoder.value, present.12.encoder.key, present.2.decoder.value)\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[\\u2713] (2, 16, 96103) matches (2, 16, 96103)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[x] values not close enough, max diff: 1.3828277587890625e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[x] values not close enough, max diff: 1.52587890625e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[x] values not close enough, max diff: 2.09808349609375e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[x] values not close enough, max diff: 1.9073486328125e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.12.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.12.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.12.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.12.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[x] values not close enough, max diff: 3.0517578125e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.13.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.13.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.13.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.13.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[x] values not close enough, max diff: 2.86102294921875e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.14.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.14.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.14.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.14.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[x] values not close enough, max diff: 2.6702880859375e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.15.decoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.15.decoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.15.encoder.key\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.15.encoder.value\":\n",
      "\t\t-[\\u2713] (2, 16, 16, 64) matches (2, 16, 16, 64)\n",
      "\t\t-[x] values not close enough, max diff: 4.38690185546875e-05 (atol: 1e-05)\n",
      "Validation 0 for the model model/pegasus/encoder_model.onnx raised: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
      "- last_hidden_state: max diff = 0.00022137165069580078\n",
      "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
      "- present.4.encoder.key: max diff = 1.3828277587890625e-05\n",
      "- present.5.encoder.value: max diff = 1.52587890625e-05\n",
      "- present.10.encoder.value: max diff = 2.09808349609375e-05\n",
      "- present.11.encoder.value: max diff = 1.9073486328125e-05\n",
      "- present.12.encoder.value: max diff = 3.0517578125e-05\n",
      "- present.13.encoder.value: max diff = 2.86102294921875e-05\n",
      "- present.14.encoder.value: max diff = 2.6702880859375e-05\n",
      "- present.15.encoder.value: max diff = 4.38690185546875e-05.\n",
      " The exported model was saved at: model/pegasus\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx --model google/pegasus-xsum --task summarization ./model/pegasus/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:04:58.555531300Z",
     "start_time": "2023-11-26T20:03:46.751795400Z"
    }
   },
   "id": "6700460879974d4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then load onnx model and its tokenizer to make some tests."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49576c272a11fdef"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/pegasus/\")\n",
    "model = ORTModelForSeq2SeqLM.from_pretrained(\"./model/pegasus/\", use_cache=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:05:03.346445200Z",
     "start_time": "2023-11-26T20:04:58.554529Z"
    }
   },
   "id": "664981c0bd7f3360"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Forward propagation test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "daacb8432b12c278"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly we measure how much time is needed for forward propagation for ONNX pegasus model with max-size input."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46fb5b8705c59a06"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "148.8189069"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_time_evaluation(model, tokenizer, dataset['validation'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:07:32.180658100Z",
     "start_time": "2023-11-26T20:05:03.342447Z"
    }
   },
   "id": "f4288230010f053c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we see above, it takes 160 seconds for ONNX model to run 103 inputs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "634c20ea7bd0a5c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb95040a6896df23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we build the summarization pipeline to test how much time it take to generate a summary for the given text:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e921f223f1753430"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "pipe = pipeline('summarization', model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:07:32.181938500Z",
     "start_time": "2023-11-26T20:07:32.176298400Z"
    }
   },
   "id": "6eca0e11687326ea"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "244"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(dataset['validation']['dialogue'][0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:07:32.228961300Z",
     "start_time": "2023-11-26T20:07:32.186241300Z"
    }
   },
   "id": "9763d7ef9e81ce35"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'The following is a transcript of a phone call between Tom and his son, Lemmy.'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": "15.870781799999804"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "print(pipe(dataset['validation']['dialogue'][0]))\n",
    "end_time = time.perf_counter()\n",
    "end_time - start_time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:07:48.086558200Z",
     "start_time": "2023-11-26T20:07:32.200795200Z"
    }
   },
   "id": "d06126426b61cc48"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "154"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(dataset['validation']['dialogue'][1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:07:48.086558200Z",
     "start_time": "2023-11-26T20:07:48.079550900Z"
    }
   },
   "id": "37ea780bce94d488"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'A conversation between a waiter and a customer in a restaurant.'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": "6.953937300000234"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "print(pipe(dataset['validation']['dialogue'][1]))\n",
    "end_time = time.perf_counter()\n",
    "end_time - start_time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:07:55.046561400Z",
     "start_time": "2023-11-26T20:07:48.084554100Z"
    }
   },
   "id": "d4c32eb68f484c0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In conclusion we can see ONNX model has good performance on given texts. It took 15.8 seconds for the first text and 7 for the second."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf7f04348eed2759"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ONNX with quantization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e313f9cd31fb6a67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we quantize our models with `quantize_dynamic`. Also, it should be mentioned that we copy [`config.json`, `generation_config.json`, `special_tokens_map.json`, `spiece.model`, `tokenizer.json`, `tokenizer_config.json`] from `./model/pegasus/` to `./model/q_pegasus/`, because we change only models (encoder, decoder), but we don't use all other utilities."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d22b255fc922384"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.0/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.0/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.0/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.0/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.1/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.1/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.1/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.1/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.2/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.2/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.2/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.2/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.3/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.3/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.3/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.3/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.4/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.4/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.4/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.4/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.5/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.5/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.5/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.5/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.6/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.6/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.6/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.6/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.7/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.7/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.7/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.7/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.8/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.8/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.8/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.8/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.9/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.9/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.9/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.9/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.10/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.10/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.10/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.10/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.11/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.11/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.11/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.11/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.12/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.12/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.12/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.12/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.13/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.13/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.13/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.13/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.14/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.14/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.14/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.14/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.15/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.15/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.15/encoder_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/model/decoder/layers.15/encoder_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/lm_head/MatMul]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/layers.0/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.0/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.1/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.1/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.2/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.2/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.3/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.3/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.4/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.4/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.5/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.5/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.6/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.6/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.7/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.7/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.8/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.8/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.9/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.9/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.10/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.10/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.11/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.11/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.12/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.12/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.13/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.13/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.14/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.14/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/layers.15/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/layers.15/self_attn/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic\n",
    "\n",
    "quantize_dynamic(\"./model/pegasus/decoder_model.onnx\", \"./model/q_pegasus/decoder_model.onnx\")\n",
    "quantize_dynamic(\"./model/pegasus/encoder_model.onnx\", \"./model/q_pegasus/encoder_model.onnx\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:08:59.459823Z",
     "start_time": "2023-11-26T20:07:55.048562400Z"
    }
   },
   "id": "4dfc7809f69d7366"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we load our quantized model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a4e56d4d8447015"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/q_pegasus/\")\n",
    "model = ORTModelForSeq2SeqLM.from_pretrained(\"./model/q_pegasus/\", use_cache=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:09:01.782669700Z",
     "start_time": "2023-11-26T20:08:59.460827200Z"
    }
   },
   "id": "880ed45907f6be15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are performing same test actions in all cells below but with quantized model. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91f508f464863281"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Forward propagation test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15c3756c577a1c1e"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "101.02335279999988"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_time_evaluation(model, tokenizer, dataset['validation'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:10:42.825962200Z",
     "start_time": "2023-11-26T20:09:01.782669700Z"
    }
   },
   "id": "40ed7da05beba2f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see inference time of quantized model is much lower than of ONNX model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87c678ef5e1b3142"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8e80fca5e1814d2"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "pipe = pipeline('summarization', model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:10:42.875754200Z",
     "start_time": "2023-11-26T20:10:42.821456400Z"
    }
   },
   "id": "f2f85107f33dd4e9"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \"Here's a letter from Lemmy to his father, Tom, explaining why he wants to buy his son a puppy.\"}]\n"
     ]
    },
    {
     "data": {
      "text/plain": "11.485970299999735"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "print(pipe(dataset['validation']['dialogue'][0]))\n",
    "end_time = time.perf_counter()\n",
    "end_time - start_time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:10:54.353276400Z",
     "start_time": "2023-11-26T20:10:42.854217900Z"
    }
   },
   "id": "625e35cb5e3ef332"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'A conversation between a waiter and a customer in a restaurant.'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": "5.546223100000134"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "print(pipe(dataset['validation']['dialogue'][1]))\n",
    "end_time = time.perf_counter()\n",
    "end_time - start_time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:10:59.902890300Z",
     "start_time": "2023-11-26T20:10:54.348276300Z"
    }
   },
   "id": "da0f6da3839bb21a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In conclusion, we can that quantized model generating pipeline with same level of performance is almost 1.5 faster."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffd321e942c9caeb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model deployment\n",
    "\n",
    "In `fast-api-service` folder we have a fastapi service that implements API to access our summarization model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a29da4f497a4ff57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below we build a docker image of fastapi service and run the container."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfbbaebbd5943960"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 building with \"default\" instance using docker driver\n",
      "\n",
      "#1 [transformer-model internal] load .dockerignore\n",
      "#1 transferring context: 2B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [transformer-model internal] load build definition from Dockerfile\n",
      "#2 transferring dockerfile: 560B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [transformer-model internal] load metadata for docker.io/tiangolo/uvicorn-gunicorn-fastapi:python3.10\n",
      "#3 ...\n",
      "\n",
      "#4 [transformer-model auth] tiangolo/uvicorn-gunicorn-fastapi:pull token for registry-1.docker.io\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#3 [transformer-model internal] load metadata for docker.io/tiangolo/uvicorn-gunicorn-fastapi:python3.10\n",
      "#3 DONE 2.1s\n",
      "\n",
      "#5 [transformer-model 1/6] FROM docker.io/tiangolo/uvicorn-gunicorn-fastapi:python3.10@sha256:8b237d01ded686ca6c2910af28302cc1997fac8adc69a10cddae603d05c4f485\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [transformer-model internal] load build context\n",
      "#6 transferring context: 692B done\n",
      "#6 DONE 0.0s\n",
      "\n",
      "#7 [transformer-model 2/6] RUN curl -sSL https://install.python-poetry.org | python - --version 1.2.0 &&     cd /usr/local/bin &&     ln -s /opt/poetry/bin/poetry &&     poetry config virtualenvs.create false\n",
      "#7 CACHED\n",
      "\n",
      "#8 [transformer-model 3/6] COPY ./pyproject.toml /app/\n",
      "#8 CACHED\n",
      "\n",
      "#9 [transformer-model 5/6] COPY service service\n",
      "#9 CACHED\n",
      "\n",
      "#10 [transformer-model 4/6] RUN poetry lock --no-update && poetry install --no-root --no-dev\n",
      "#10 CACHED\n",
      "\n",
      "#11 [transformer-model 6/6] COPY model model\n",
      "#11 CACHED\n",
      "\n",
      "#12 [transformer-model] exporting to image\n",
      "#12 exporting layers done\n",
      "#12 writing image sha256:1a5c007ee061ad7acdf8617a8a0bc7d15d83f800d9aee221dcb40e96656f92d4 done\n",
      "#12 naming to docker.io/library/transformer-service:latest done\n",
      "#12 DONE 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container transformer-service-latest  Created\n",
      " Container transformer-service-latest  Starting\n",
      " Container transformer-service-latest  Started\n"
     ]
    }
   ],
   "source": [
    "!cd fast-api-service && docker-compose build && docker-compose up -d"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:11:03.575483400Z",
     "start_time": "2023-11-26T20:10:59.898880100Z"
    }
   },
   "id": "d4d164d5f7bd4cfb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will check the operation of the API."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6db65898a250580d"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'{\"summary\":\"On this week\\'s episode of Corriere dello Sport, we take a look at the relationship between Matteo and Gosia.\"}'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.post(\n",
    "    'http://127.0.0.1:8088/summarize',\n",
    "    data=\"Giuseppe: Hi man Matteo: Yo Giuseppe: How's it going with Gosia? Matteo: I don't know, she's a little strange Giuseppe: Why? Matteo: She always criticizes me because I like football and video games Giuseppe: Damn Matteo: Yeah... Giuseppe: Ok, I don't like games either, but... Matteo: You boring guy Giuseppe: Lol Matteo: Anyway I like her a lot Giuseppe: I can understand that, she's hot, if you ever dump her make sure you tell me Matteo: Get your hands off her, man Giuseppe: Just kidding Matteo: Lollolol\"\n",
    "    ).text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T20:11:42.411068300Z",
     "start_time": "2023-11-26T20:11:34.332465Z"
    }
   },
   "id": "67cc5c01b7f1853"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "requests.post(\n",
    "    'http://127.0.0.1:8088/summarize',\n",
    "    data=dataset['validation']['dialogue'][0].encode('utf-8')\n",
    ").text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-26T20:11:04.918368700Z"
    }
   },
   "id": "c6d2f028d7c31a66"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we see it gives responses to our requests with expected result."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5350cc7801ffe343"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
