{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-10-29T16:17:01.545236Z",
     "iopub.execute_input": "2023-10-29T16:17:01.545918Z",
     "iopub.status.idle": "2023-10-29T16:17:01.929979Z",
     "shell.execute_reply.started": "2023-10-29T16:17:01.545884Z",
     "shell.execute_reply": "2023-10-29T16:17:01.929026Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/input/nlp-ua-locations-extractions/ru_geo_dataset.csv\n/kaggle/input/nlp-ua-locations-extractions/README.md\n/kaggle/input/nlp-ua-locations-extractions/labeling_sample.csv\n/kaggle/input/nlp-ua-locations-extractions/test.csv\n/kaggle/input/nlp-ua-locations-extractions/uk_geo_dataset.csv\n/kaggle/input/nlp-ua-locations-extractions/uk_geo_dataset_processed_v1.parquet\n/kaggle/input/data-for-training/small_valid_processed.json\n/kaggle/input/data-for-training/small_train_processed.json\n/kaggle/input/tokenized-data/small_valid_processed.json\n/kaggle/input/tokenized-data/small_train_processed.json\n/kaggle/input/tokenized-data/valid_processed.json\n/kaggle/input/medium-dataset/medium_train_processed.json\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import re"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-29T16:17:01.931941Z",
     "iopub.execute_input": "2023-10-29T16:17:01.932746Z",
     "iopub.status.idle": "2023-10-29T16:17:01.936817Z",
     "shell.execute_reply.started": "2023-10-29T16:17:01.932710Z",
     "shell.execute_reply": "2023-10-29T16:17:01.935771Z"
    },
    "trusted": true
   },
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove links\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "\n",
    "    # Special remove telegram links\n",
    "    pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:t\\.me\\/\\S+|telegram\\.me\\/\\S+|telegram\\.dog\\/\\S+)\"\n",
    "    text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Remove phone numbers\n",
    "    phone_regex = r'\\(?\\+?\\d{0,3}\\)?[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{2}[-.\\s]?\\d{2}'\n",
    "    text = re.sub(phone_regex, '', text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[\\n\\t\\r]', ' ', text)\n",
    "\n",
    "    # Remove tags\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        pattern=\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "\n",
    "    return text"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-29T16:17:01.937991Z",
     "iopub.execute_input": "2023-10-29T16:17:01.938343Z",
     "iopub.status.idle": "2023-10-29T16:17:01.949713Z",
     "shell.execute_reply.started": "2023-10-29T16:17:01.938310Z",
     "shell.execute_reply": "2023-10-29T16:17:01.948917Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install evaluate\n",
    "!pip install seqeval\n",
    "!pip install sacremoses"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-29T16:17:01.951583Z",
     "iopub.execute_input": "2023-10-29T16:17:01.951881Z",
     "iopub.status.idle": "2023-10-29T16:17:43.399729Z",
     "shell.execute_reply.started": "2023-10-29T16:17:01.951856Z",
     "shell.execute_reply": "2023-10-29T16:17:43.398768Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001B[2K     \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m3.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001B[2K     \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m43.6/43.6 kB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.23.5)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=9ee702402bdbf2c15f5bd90bf5c4dcf9791dd5932cb9b9a79660aa5638ab7186\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\nCollecting sacremoses\n  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n\u001B[2K     \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m880.6/880.6 kB\u001B[0m \u001B[31m18.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacremoses) (2023.6.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from sacremoses) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses) (1.3.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sacremoses) (4.66.1)\nBuilding wheels for collected packages: sacremoses\n  Building wheel for sacremoses (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=f3779d0e9bd1ebd1e8c4ec75c6ff53ae01f12c7e0620f59d3ff137eb48412024\n  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\nSuccessfully built sacremoses\nInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.0.53\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model_name = 'xlm-mlm-100-1280'\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "label2id = {'O': 0, 'B-LOC': 1, 'I-LOC': 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "label_names = list(label2id.keys())\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-29T16:17:43.401209Z",
     "iopub.execute_input": "2023-10-29T16:17:43.401518Z",
     "iopub.status.idle": "2023-10-29T16:18:00.343400Z",
     "shell.execute_reply.started": "2023-10-29T16:17:43.401491Z",
     "shell.execute_reply": "2023-10-29T16:18:00.342547Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41bcb24db1c0470b839682997af7cbd3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27bdf8ca552b413b8f99815344b8ba6c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12692728a85449638d25cf51c6da03a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81ff2d2e421b43058d50602b167e43a1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc0eed2e5d6f4cdb905cbe93b9a94147"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "\n",
    "\n",
    "raw_datasets_ua = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        'train': '/kaggle/input/medium-dataset/medium_train_processed.json',\n",
    "        'val': '/kaggle/input/tokenized-data/valid_processed.json'\n",
    "    }\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "tokenized_datasets_ua = raw_datasets_ua.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets_ua[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(\n",
    "    [\n",
    "        {'params': list(model.bert.parameters()), 'lr': 1e-5},\n",
    "        {'params': list(model.classifier.parameters()), 'lr': 1e-3}\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_batch_size = 16\n",
    "val_batch_size = 16\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0.1 * 3 * (tokenized_datasets_ua['train'].num_rows / train_batch_size),\n",
    "    num_training_steps=3 * (tokenized_datasets_ua['train'].num_rows / val_batch_size)\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-ua-loc-ner\",\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=val_batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    no_cuda=False,\n",
    "\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-29T16:19:17.127538Z",
     "iopub.execute_input": "2023-10-29T16:19:17.127928Z",
     "iopub.status.idle": "2023-10-29T16:20:42.712567Z",
     "shell.execute_reply.started": "2023-10-29T16:19:17.127901Z",
     "shell.execute_reply": "2023-10-29T16:20:42.711619Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-03d45eb6c6ece586/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46f6c5198d2c4847a653f01eeb92f193"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66f0ef3c38d84cb486d6610c89401006"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-03d45eb6c6ece586/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3f1db907bb1456da25aecdd6f41264e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7cae229acd945769d8b0885361165f5"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c153ea71b2244eb69b68470132aa176a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/14 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e503a70d32d04ca6af2c827146dc24a0"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets_ua[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_ua[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-29T16:20:42.714309Z",
     "iopub.execute_input": "2023-10-29T16:20:42.714603Z",
     "iopub.status.idle": "2023-10-29T16:20:42.724010Z",
     "shell.execute_reply.started": "2023-10-29T16:20:42.714577Z",
     "shell.execute_reply": "2023-10-29T16:20:42.723236Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-29T16:20:42.724997Z",
     "iopub.execute_input": "2023-10-29T16:20:42.725363Z",
     "iopub.status.idle": "2023-10-29T19:41:59.400897Z",
     "shell.execute_reply.started": "2023-10-29T16:20:42.725339Z",
     "shell.execute_reply": "2023-10-29T19:41:59.399425Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
     "output_type": "stream"
    },
    {
     "output_type": "stream",
     "name": "stdin",
     "text": "  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
    },
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.9"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20231029_162255-fsp2ezwl</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/birdwithdreams/huggingface/runs/fsp2ezwl' target=\"_blank\">happy-morning-6</a></strong> to <a href='https://wandb.ai/birdwithdreams/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/birdwithdreams/huggingface' target=\"_blank\">https://wandb.ai/birdwithdreams/huggingface</a>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/birdwithdreams/huggingface/runs/fsp2ezwl' target=\"_blank\">https://wandb.ai/birdwithdreams/huggingface/runs/fsp2ezwl</a>"
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='14287' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14287/18750 3:18:27 < 1:02:00, 1.20 it/s, Epoch 2.29/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.007100</td>\n      <td>0.015691</td>\n      <td>0.900445</td>\n      <td>0.905781</td>\n      <td>0.903105</td>\n      <td>0.996767</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.005100</td>\n      <td>0.013596</td>\n      <td>0.902842</td>\n      <td>0.918239</td>\n      <td>0.910476</td>\n      <td>0.996994</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1553\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1554\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1555\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1556\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1557\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1558\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1835\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1832\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m   1834\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[0;32m-> 1835\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1837\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1838\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   1839\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[1;32m   1840\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   1841\u001B[0m ):\n\u001B[1;32m   1842\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   1843\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2690\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   2688\u001B[0m         scaled_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m   2689\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2690\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2692\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach() \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:1921\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[0;34m(self, loss, **kwargs)\u001B[0m\n\u001B[1;32m   1919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   1920\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1921\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1922\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1923\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"./bert-ua-loc-ner/checkpoint-12500/\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-29T19:42:21.150247Z",
     "iopub.execute_input": "2023-10-29T19:42:21.150624Z",
     "iopub.status.idle": "2023-10-29T19:42:23.250663Z",
     "shell.execute_reply.started": "2023-10-29T19:42:21.150594Z",
     "shell.execute_reply": "2023-10-29T19:42:23.249597Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv('/kaggle/input/nlp-ua-locations-extractions/test.csv')\n",
    "test_df.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-29T19:42:23.252958Z",
     "iopub.execute_input": "2023-10-29T19:42:23.253548Z",
     "iopub.status.idle": "2023-10-29T19:42:23.297745Z",
     "shell.execute_reply.started": "2023-10-29T19:42:23.253507Z",
     "shell.execute_reply": "2023-10-29T19:42:23.296835Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "execution_count": 11,
     "output_type": "execute_result",
     "data": {
      "text/plain": "   text_id                                               text locations\n0        0  ‚ùóÔ∏è–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Ä–∞–Ω–µ–Ω–∏—Ö –∑—Ä–æ—Å–ª–∞ –¥–æ —Ç—Ä—å–æ—Ö, ‚Äì –ö–ª–∏—á–∫...        []\n1        1  ü•§–í –ö–∏—î–≤—ñ –∑–∞ 91,13 –º–ª–Ω –≥—Ä–∏–≤–µ–Ω—å –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è —Ä–æ–±–æ...        []\n2        2  ‚ñ™Ô∏è–°—å–æ–≥–æ–¥–Ω—ñ –≤–Ω–æ—á—ñ —Ä–æ—Å—ñ—è–Ω–∏ –∑–∞–≤–¥–∞–ª–∏ —Ä–∞–∫–µ—Ç–Ω–æ–≥–æ —É–¥–∞...        []\n3        3  –ù–∞—Ä–∞–∑—ñ —É –∑–∞–ø–∞—Å–∞—Ö —Ä–æ—Å—ñ—è–Ω –Ω–∞–π–±—ñ–ª—å—à–µ –±–∞–ª—ñ—Å—Ç–∏—á–Ω–∏—Ö ...        []\n4        4  ‚õ∏–í –æ–¥–∏–Ω –¥–µ–Ω—å, 29 —Å–µ—Ä–ø–Ω—è, –î–ü \"–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞ —É—á–±–æ–≤–æ...        []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>text</th>\n      <th>locations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>‚ùóÔ∏è–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Ä–∞–Ω–µ–Ω–∏—Ö –∑—Ä–æ—Å–ª–∞ –¥–æ —Ç—Ä—å–æ—Ö, ‚Äì –ö–ª–∏—á–∫...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>ü•§–í –ö–∏—î–≤—ñ –∑–∞ 91,13 –º–ª–Ω –≥—Ä–∏–≤–µ–Ω—å –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è —Ä–æ–±–æ...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>‚ñ™Ô∏è–°—å–æ–≥–æ–¥–Ω—ñ –≤–Ω–æ—á—ñ —Ä–æ—Å—ñ—è–Ω–∏ –∑–∞–≤–¥–∞–ª–∏ —Ä–∞–∫–µ—Ç–Ω–æ–≥–æ —É–¥–∞...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>–ù–∞—Ä–∞–∑—ñ —É –∑–∞–ø–∞—Å–∞—Ö —Ä–æ—Å—ñ—è–Ω –Ω–∞–π–±—ñ–ª—å—à–µ –±–∞–ª—ñ—Å—Ç–∏—á–Ω–∏—Ö ...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>‚õ∏–í –æ–¥–∏–Ω –¥–µ–Ω—å, 29 —Å–µ—Ä–ø–Ω—è, –î–ü \"–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞ —É—á–±–æ–≤–æ...</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_df['clean_text'] = test_df['text'].apply(preprocess_text)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-29T19:42:23.298803Z",
     "iopub.execute_input": "2023-10-29T19:42:23.299056Z",
     "iopub.status.idle": "2023-10-29T19:42:23.453651Z",
     "shell.execute_reply.started": "2023-10-29T19:42:23.299032Z",
     "shell.execute_reply": "2023-10-29T19:42:23.452546Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def predict(text):\n",
    "    locs = token_classifier(text)\n",
    "    locs = [loc['word'] for loc in locs]\n",
    "    locs = [loc for loc in locs if not loc.startswith('#')]\n",
    "    return locs"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-29T19:42:23.455731Z",
     "iopub.execute_input": "2023-10-29T19:42:23.456013Z",
     "iopub.status.idle": "2023-10-29T19:42:23.462212Z",
     "shell.execute_reply.started": "2023-10-29T19:42:23.455987Z",
     "shell.execute_reply": "2023-10-29T19:42:23.461143Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "preds = test_df['clean_text'].apply(predict)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-29T19:42:23.463729Z",
     "iopub.execute_input": "2023-10-29T19:42:23.463988Z",
     "iopub.status.idle": "2023-10-29T19:43:36.818218Z",
     "shell.execute_reply.started": "2023-10-29T19:42:23.463965Z",
     "shell.execute_reply": "2023-10-29T19:43:36.817149Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame({\n",
    "    'text_id': test_df['text_id'],\n",
    "    'locations': preds\n",
    "})\n",
    "df.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-29T19:43:36.820109Z",
     "iopub.execute_input": "2023-10-29T19:43:36.820632Z",
     "iopub.status.idle": "2023-10-29T19:43:36.835026Z",
     "shell.execute_reply.started": "2023-10-29T19:43:36.820592Z",
     "shell.execute_reply": "2023-10-29T19:43:36.834089Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "execution_count": 15,
     "output_type": "execute_result",
     "data": {
      "text/plain": "   text_id                                          locations\n0        0                                                 []\n1        1  [–ö–∏—î–≤—ñ, –®—É–ª—è–≤—Å—å–∫–æ–≥–æ —à–ª—è—Ö–æ–ø—Ä–æ–≤–æ–¥—É, –®—É–ª—è–≤—Å—å–∫–æ–≥–æ ...\n2        2  [–ì–æ–≥–æ–ª–µ–≤–µ, –ú–∏—Ä–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ —Ä–∞–π–æ–Ω—É, –ü–æ–ª—Ç–∞–≤—Å—å–∫–æ—ó ...\n3        3                                                 []\n4        4                  [–ø—Ä–æ—Å–ø–µ–∫—Ç—ñ –ê–∫–∞–¥–µ–º—ñ–∫–∞ –ì–ª—É—à–∫–æ–≤–∞, 9]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>locations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[–ö–∏—î–≤—ñ, –®—É–ª—è–≤—Å—å–∫–æ–≥–æ —à–ª—è—Ö–æ–ø—Ä–æ–≤–æ–¥—É, –®—É–ª—è–≤—Å—å–∫–æ–≥–æ ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[–ì–æ–≥–æ–ª–µ–≤–µ, –ú–∏—Ä–≥–æ—Ä–æ–¥—Å—å–∫–æ–≥–æ —Ä–∞–π–æ–Ω—É, –ü–æ–ª—Ç–∞–≤—Å—å–∫–æ—ó ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[–ø—Ä–æ—Å–ø–µ–∫—Ç—ñ –ê–∫–∞–¥–µ–º—ñ–∫–∞ –ì–ª—É—à–∫–æ–≤–∞, 9]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df.to_csv('/kaggle/working/submission2.csv', index=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-29T19:43:36.836746Z",
     "iopub.execute_input": "2023-10-29T19:43:36.837631Z",
     "iopub.status.idle": "2023-10-29T19:43:36.852712Z",
     "shell.execute_reply.started": "2023-10-29T19:43:36.837593Z",
     "shell.execute_reply": "2023-10-29T19:43:36.851778Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The submission score is 0.4860."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
